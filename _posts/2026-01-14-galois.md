---
layout: post
title:  "Quick Intro to Galois Theory"
date:   2026-01-14
categories: [math]
mathjax: true
---

# Polynomials and Symmetry of Roots

#### Solving the polynomial

Consider a polynomial

$$f(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0$$

All the
coefficients $a_n, a_{n-1}, a_{n-2}, \ldots, a_2, a_1, a_0$ are known.
By the fundamental theorem of algebra, the polynomial can be factored in
terms of its $n$ roots:

$$f(x) = a_n (x - r_1) (x-r_2)\ldots(x-r_{n-1})(x-r_n)$$

These roots
$r_1, r_2, \ldots r_{n-1}, r_n$ are unknown and we would like to find
them in terms of the coefficients of the polynomial.

#### Vieta's Formula

The coefficients of the polynomial and the roots of the polynomial are
related:

$$f(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0 = a_n (x - r_1) (x-r_2)\ldots(x-r_{n-1})(x-r_n)$$

By multiplying out the product on the right side, we get:

$$r_1  + r_2 + r_3 \ldots + r_{n-1} + r_n = -\frac{a_{n-1}}{a_n}$$

$$r_1r_2  + r_1r_3 + \ldots + r_{n-1}r_n = \frac{a_{n-2}}{a_n}$$

$$\vdots$$

$$r_1r_2 \ldots r_{n-1}r_n = \frac{a_{0}}{a_n}$$

In other
words, the $n$ elementary symmetric polynomial expressions in roots are
known. We need to solve for the individual roots.

#### Newton's Theorem

Any symmetric polynomial in $n$ variables can be uniquely expressed as a
polynomial in the $n$ elementary symmetric polynomials in the same $n$
variables. This can be proved using induction in the number of
variables.

Here is a rough sketch of proof: Consider the symmetric polynomial

$$p(a,b,c) = a^2 + b^2 + c^2$$

This can be expressed as a polynomial in
$c$ whose coefficients are symmetric polynomials in $a, b$. Assume that
we knew how to express any symmetric polynomial in 2 variables in terms
of its elementary symmetric polynomials: 

$$t_1 = a + b$$

$$t_2 = ab$$

This gives

$$p(a,b,c) = c^2 + (a^2 + b^2) = c^2 + (t_1^2 -2t_2)$$

Now, we have a
polynomial in $c$ whose coefficients are polynomials in $t_1, t_2$.

We know that the elementary symmetric polynomials in 3 variables are:

$$e_1 = a+b+c = t_1 + c$$

$$e_2 = ab+bc+ca = ab  + c(a+b) = t_2 + ct_1$$

$$e_3 = abc = ct_2$$

These can also be written as: 

$$t_1  = e_1 - c$$

$$t_2  = e_2 - ct_1 = e_2 - ce_1 + c^2$$

$$c^3 = e_3  - ce_2 + c^2e_1$$

$p(a,b,c) = c^2 + (a^2 + b^2) = c^2 + (t_1^2 -2t_2)$. Using the first
two expressions, $t_1, t_2$ can be written in terms of $e_1, e_2, e_3$
and $c$. Using the third expression, the degree of $c$ in the resulting
expression can be reduced to $2$. This gives a polynomial in $c$ whose
coefficients are $e_1, e_2, e_3$, with degree atmost $2$.

Since $p(a,b,c)$ is known to be a symmetric polynomial in $a,b,c$, the
variable $c$ can be replaced with any one of $a, b$, or $c$. Thus, the
polynomial expression is a degree $2$ equation with $3$ roots. Hence, it
is a constant polynomial, without any terms with powers of $c$.

#### Symmetric Polynomial in Roots

For the original polynomial
$f(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0$ with roots
$r_1, r_2,\ldots, r_{n-1}, r_n$, we can evaluate all symmetric
polynomials in $r_1, r_2,\ldots, r_{n-1}, r_n$. We need to find the
values of the individual roots.

# Solving Polynomials upto degree 4

#### Quadratic Polynomial

Consider the polynomial $f(x) = x^2 + ax + b$. It can be solved by
completing the square:

$$f(x) = x^2 + ax + b  = \big ( x+\frac{a}{2} \big )^2 - \frac{a^2}{4} + b=0$$

Substituting, $u = x+a/2$, the polynomial becomes

$$u^2 - \big ( \frac{a^2-4b}{4} \big ) = 0$$

This can be solved using a square root.

**What this means**: Instead of solving for a polynomial whose roots are
$r_1, r_2$, it is easier to first solve a polynomial whose roots are
$r_1 + a/2 = (r_1 - r_2)/2$ and $r_2  + a/2 = (r_2 - r_1)/2$. The
coefficients of such a polynomial can always be found using known values
(the coefficients of the original polynomial) and the polynomial is
solvable using a square root alone.

#### Cubic Polynomial

Consider the polynomial $f(x) = x^3 + ax^2 + bx + c$. Substituting,
$y = x+a/3$ completes the cube, removing the second degree term. This
turns the polynomial into the depressed form: $y^3 + py + q=0$.

Substitute $y = u+v$. This turns the polynomial into
$u^3 +v^3 + (3uv+p)(u+v) + q=0$. Let $uv=-p/3$. This turns the
polynomial into $u^3 +v^3 = -q$. Thus, we know the sum and product of
$u^3$ and $v^3$. This can be solved using a quadratic equation. This
gives two solutions, one corresponding to $(u^3, v^3)$ and the other
corresponding to $(v^3, u^3)$. We can choose one of these without loss
of generality.

Once we know $u^3$, we can get the three cube roots of $u^3$:
$u, u\omega, u\omega^2$ where $\omega$ is the primitive cube root of
unity. From $uv=-p/3$, we can get the corresponding value of $v$ for
each case: $(u,v)$, $(u\omega, v\omega^2)$, $(u\omega^2, v\omega)$. This
gives $3$ roots of the depressed cubic: $y=u + v$,
$u\omega +  v\omega^2$, $u\omega^2 +  v\omega$.

**What this means**: Instead of solving for the cubic with roots $r_1$,
$r_2$, $r_3$, we can instead solve the depressed cubic with roots
$r_1+a/3$, $r_2+a/3$, $r_3+a/3$ with $a = -(r_1+r_2+r_3)$. Further,
instead of solving for this depressed cubic, we can instead solve for a
quadratic whose roots are $u, v$ which satisfy:

$$
\begin{align*}
r_1+a/3 &= u + v\\
r_2+a/3 &= u\omega +  v\omega^2\\
r_3+a/3 &= u\omega^2 +  v\omega
\end{align*}
$$

which can also be expressed as:

$$3u = r_1 + r_2\omega^2 + r_3\omega$$

$$3v = r_1 + r_2\omega + r_3\omega^2$$

Hence, it is always possible to create a quadratic with roots
$r_1 + r_2\omega^2 + r_3\omega$ and $r_1 + r_2\omega + r_3\omega^2$
whose coefficients are known values (can be expressed in terms of the
coefficients of the original polynomial). Solving this, we can obtain
the individual roots from the system of equations for $u, v$ and
$r_1+r_2+r_3 = -a$.

#### Quartic Polynomial

Consider the polynomial $f(x) = x^4 + ax^3 + bx^2 + cx + d$.
Substituting, $y = x+a/4$ removes the third degree term. This turns the
polynomial into the depressed form: $y^4 + py^2 + qy + r=0$. We can
rewrite this as:

$$y^4 = -py^2 - qy - r$$

Adding $m^2 + 2my^2$ on both sides, we can try
making both sides perfect squares:

$$(y^2 + m)^2= (2m-p)y^2 - qy + (m^2- r)$$

The left side is a perfect
square. The right side is a quadratic in $y$. For the right side to be a
perfect square, we need the discriminant to be zero:

$$q^2 = 4(2m-p)(m^2- r)$$

This is a cubic in $m$ that can be solved.
Using any one of these $m$'s, we can write the equation as equality of
$2$ squares:

$$(y^2 + m)^2= (\sqrt{2m-p}y - \frac{q}{2\sqrt{2m-p}})^2$$

This breaks down into two possible quadratics in $y$ that can be solved.

$$(y^2 + m)= \pm (\sqrt{2m-p}y - \frac{q}{2\sqrt{2m-p}})$$

The quadratics are:

$$y^2  + \sqrt{2m-p}y + m-\frac{q}{2\sqrt{2m-p}} = 0$$

$$y^2 -\sqrt{2m-p}y + m + \frac{q}{2\sqrt{2m-p}} = 0$$

**What this means**: Instead of solving for the quartic with roots
$r_1$, $r_2$, $r_3$, $r_4$ we can instead solve the depressed quartic
with roots $t_1 = r_1+a/4$, $t_2=r_2+a/4$, $t_3=r_3+a/4$, $t_4=r_4+a/4$
with $a = -(r_1+r_2+r_3+r_4)$. Further, instead of solving for this
depressed quartic, we can instead solve a cubic in $m$ that satisfies:

$$t_1 + t_2 = -\sqrt{2m-p}$$

$$t_3 + t_4 = +\sqrt{2m-p}$$

Multiplying
these we get:

$$t_1t_3 + t_1t_4 + t_2t_3 + t_2t_4 = p-2m$$

Since
$p = \sum t_it_j$, this simplifies to:

$$2m = t_1t_2 + t_3t_4$$

Writing
this in form of $r_i\text{s}$:

$$2m = r_1r_2 + r_3r_4 - a^2/8$$

Hence, it is equivalent to say that instead of solving the depressed
quartic, we can solve a cubic whose roots are $r_1r_2 + r_3r_4$,
$r_1r_3 + r_2r_4$, $r_1r_4 + r_2r_3$.

If we know $r_1r_2 + r_3r_4$, we can find $(r_1r_2, r_3r_4)$ by solving
a quadratic, since we know both the sum and product
($r_1r_2r_3r_4 = d$). Also, if we know $r_1r_2 + r_3r_4$, we can also
find $(r_1+r_2)(r_3 + r_4)  = b - (r_1r_2 + r_3r_4)$. Once again, by the
same logic, we can find $(r_1+r_2, r_3 + r_4)$ by solving a quadratic,
since we know the sum ($r_1+r_2 + r_3 + r_4 = -a$) and product.

We know $(r_1+r_2, r_3 + r_4)$ and $(r_1r_2, r_3r_4)$. From these we can
solve for $(r_1, r_2, r_3, r_4)$ using quadratics. However, to do this,
we need to know which sum corresponds to which product. Both of these
are unordered pairs of numbers. For $r_1+r_2$, we need to know which of
the two numbers $(r_1r_2, r_3r_4)$ is actually $r_1r_2$. We can do this
by constructing the two possible values for:

$$\frac{r_1 + r_2}{r_1r_2} + \frac{r_3 + r_4}{r_3r_4} = \frac{1}{r_1} + \frac{1}{r_2} + \frac{1}{r_3} + \frac{1}{r_4}$$

Since the right side is symmetric in $r_1$, $r_2$, $r_3$, $r_4$, this is
a known quantity. Only one of the two possible values that we can come
up by matching $(r_1+r_2, r_3 + r_4)$ with $(r_1r_2, r_3r_4)$ is the
correct value. This can help us identify the correct way to match the
sums and products, which can then help us solve for the roots using
quadratics.

#### Resolvent Polynomial

Here is a summary of the above methods:

1.  **Quadratic**: We first solve a polynomial with roots
    $t = r_1 - r_2$ under all possible permutations of $r_1$, $r_2$.
    This polynomial can be expressed in terms of known quantities and is
    a quadratic that lacks the linear term. It can be solved using only
    a square root.

2.  **Cubic**: We first solve a polynomial with roots
    $t =r_1 + \omega r_2 + \omega^2 r_3$ under all possible permutations
    of $r_1$, $r_2$, $r_3$. This polynomial can be expressed in terms of
    known quantities and is a quadratic polynomial that we already know
    how to solve. Once we know the two possible values of $t$, we can
    solve for the roots $r_i$ using a system of linear equations.

3.  **Quartic**: We first solve a polynomial with roots
    $t =r_1r_2 + r_3r_4$ under all possible permutations of $r_1$,
    $r_2$, $r_3$, $r_4$. This polynomial can be expressed in terms of
    known quantities and is a cubic polynomial that we already know how
    to solve. Once we know the three possible values of
    $r_1r_2 + r_3r_4$, we can find the values $r_1r_2$ and $r_1 + r_2$
    by solving two quadratics. Further, we can solve for the individual
    roots $r_1$, $r_2$ using a quadratic. We can do this similarly for
    the other root pairs.

In all these approaches, the first step has been to identify another
polynomial that is both easier to solve and can help with finding the
roots of the original polynomial. We call this polynomial the
**"Resolvent Polynomial\"**.

# Lagrange Resolvent

#### Lagrange Resolvent

Let $\alpha$ be a primitive $n^{th}$ root of unity. For $n = 1,2,3,$ and
$4$, $\alpha$ is $1, -1, \omega,$ and $i$ respectively. We assume all of
these to be known quantities since we can express these as roots of
quadratics that we know how to solve. For example, $\omega$ is a root of
$x^2 + x+1=0$ and $i$ is a root of $x^2 + 1=0$.

Instead of solving the $n^{th}$ degree polynomial $f(x)$ with roots
$r_1$, $r_2$, $\ldots$, $r_n$, let us try solving for a polynomial
$g(x)$ whose roots are
$t = r_1 + \alpha r_2 + \alpha^2 r_3 + \ldots + \alpha^{n-2} r_{n-1} + \alpha^{n-1} r_n$
formed by the $n!$ permutations of the $n$ roots $r_1$, $r_2$, $\ldots$,
$r_n$. Hence $g(x)$ is a $n!$ degree polynomial. $t$ is called the
Lagrange Resolvent. From the previous section, we observe that this
method seems to work for degree $2$ and degree $3$ equations. Degree $4$
seems to use a similar method with a different resolvent.

#### Resolvent Polynomial

$g(x)$ is called the resolvent polynomial. For the polynomials that we
have tried solving so far, the resolvent polynomial always had
coefficients that can be expressed using known values (coefficients of
$f$). Here is why this happens. Let the roots of $g$ be $t_1$, $t_2$,
$\ldots$, $t_{n!}$. The coefficients of $g$ are elementary symmetric
polynomials in $t_i$. Any symmetric polynomial in $t_i$ is also
symmetric in $r_i\text{s}$, since permuting $r_i\text{s}$ only permutes $t_i\text{s}$. Thus,
the coefficients of $g$ are symmetric polynomials in $r_i\text{s}$ and are
hence known quantities.

For the polynomials that we have tried solving so far, the resolvent
polynomial has also been solvable. Here is how this works for degree $2$
and degree $3$ polynomials.

#### Solving quadratic with Lagrange Resolvent

For the quadratic polynomial $f(x) = (x-r_1)(x-r_2)$, the resolvents are

$$t_1 = r_1 - r_2$$

$$t_2 = r_2 - r_1$$

The resolvent polynomial
$g(x) = (x-t_1)(x-t_2)$ is also a quadratic. However, in this case,
$t_1 = -t_2$.

$$g(x) = (x-t_1)(x + t_1) = x^2 - t_1^2$$

This is a
simpler quadratic that lacks the linear term and can be solved with only
a square root. Solving this, gives two possible values of $t_1$, one
corresponding to $r_1- r_2$ and the other corresponding to $r_2- r_1$.
We can choose any one of these without loss of generality. Let us say
our choice corresponds to $t_1 = r_1 - r_2$. Since we know $r_1 + r_2$
and $r_1 - r_2$, we can solve this linear system to get $r_1$ and $r_2$.

#### Solving cubic with Lagrange Resolvent

For the cubic polynomial $f(x) = (x-r_1)(x-r_2)(x-r_3)$, the resolvents
are

$$t_1 = r_1 + \omega r_2 + \omega^2 r_3$$

$$t_2 = r_3 + \omega r_1 + \omega^2 r_2$$

$$t_3 = r_2 + \omega r_3 + \omega^2 r_1$$

$$t_4 = r_1 + \omega r_3 + \omega^2 r_2$$

$$t_5 = r_2 + \omega r_1 + \omega^2 r_3$$

$$t_6 = r_3 + \omega r_2 + \omega^2 r_1$$

The resolvent polynomial
$g(x) = \prod_{i=1}^{6} (x-t_i)$ is a $6^{th}$ degree polynomial.
However, in this case, $t_2 = \omega t_1$, $t_3 = \omega^2 t_1$ and
similarly $t_5 = \omega t_4$, $t_6 = \omega^2 t_4$.

$$g(x) = (x-t_1)(x-\omega t_1)(x-\omega^2 t_1)(x-t_4)(x-\omega t_4)(x-\omega^2 t_4)$$

$$g(x) = (x^3-t_1^3)(x^3-t_4^3)$$

where:

$$t_1 = r_1 + \omega r_2 + \omega^2 r_3$$

$$t_4 = r_1 + \omega r_3 + \omega^2 r_2$$

This is actually a quadratic
in $x^3$ and can be solved using the above method. We observe that any
odd permutation of the roots $r_1$, $r_2$, $r_3$ maps $t_1^3$ to $t_4^3$
(and vice versa). Any even permutation of the roots $r_1$, $r_2$, $r_3$
maps $t_1^3$ to $t_4^3$ itself. Hence, the two roots of the quadratic
equation $g(x) = (x^3-t_1^3)(x^3-t_4^3)$ can be assigned to
$t_1^3, t_4^3$ in any order. From this, we can recover $t_1$ using a
cube root. There are $3$ possible cube roots, which are equivalent to
cyclic permutations (or the 3 even permutations) of the roots. Once we
know $t_1$, we can recover the corresponding $t_4$ since $t_1t_4$ is a
symmetric polynomial in the roots and is hence a known quantity. From
$t_1, t_4$, $r_1 + r_2 + r_3$ (a known quantity), we can solve for the
individual roots by solving the system of linear equations.

#### Solving quartic with Lagrange Resolvent

In the previous section, we showed that the quartic can be solved using
the resolvent:

$$t_1 = r_1r_2 + r_3r_4$$

$$t_2 = r_1r_3 + r_2r_4$$

$$t_3 = r_1r_4 + r_2r_3$$

The resulting resolvent polynomial is a cubic.
Solving this, followed by solving a few quadratic equations solves the
quartic polynomial. Quartic can also be solved using the Lagrange
Resolvent $t = r_1 + ir_2 - r_3 - ir_4$. Similar to the cubic case, the
resolvent polynomial in this case can be written as:

$$g(x) = (x^4-t_1^4)(x^4-t_2^4)(x^4-t_3^4)(x^4-t_4^4)(x^4-t_5^4)(x^4-t_6^4)$$

This is a $24^{th}$ degree equation, but a $6^{th}$ degree equation in
$x^4$, where:

$$t_1 = (r_1 - r_2) + i(r_3  - r_4)$$

$$t_2 = (r_1 - r_2) + i(r_4  - r_3)$$

$$t_3 = (r_1 - r_3) + i(r_4  - r_2)$$

$$t_4 = (r_1 - r_3) + i(r_2  - r_4)$$

$$t_5 = (r_1 - r_4) + i(r_2  - r_3)$$

$$t_6 = (r_1 - r_4) + i(r_3  - r_2)$$

This is a $6^{th}$ degree
polynomial that we do not know how to solve. Instead of solving the
resolvent polynomial directly, we go on to solve other auxilary
polynomials. We observe that:

$$t_1t_2 = (r_1 - r_2)^2 + (r_3  - r_4)^2 = r_1^2 + r_2^2 + r_3^2 + r_4^2 - 2(r_1r_2 + r_3r_4)$$

$$t_3t_4 = (r_1 - r_3)^2 + (r_4  - r_2)^2 = r_1^2 + r_2^2 + r_3^2 + r_4^2 - 2(r_1r_3 + r_2r_4)$$

$$t_5t_6 = (r_1 - r_4)^2 + (r_2  - r_3)^2 = r_1^2 + r_2^2 + r_3^2 + r_4^2 - 2(r_1r_4 + r_2r_3)$$

$t_1t_2, t_3t_4, t_5t_6$ are roots of a cubic polynomial with known
coefficients and can be solved. We also observe that:

$$t_1^2 - t_2^2 = 4i(r_1 - r_2)(r_3  - r_4) = 4i(r_1r_3+r_2r_4)-4i(r_1r_4+r_2r_3)$$

$$t_1^2 - t_2^2 = 2i(t_5t_6-t_3t_4)$$

This means that once we know
$t_1t_2$, $t_3t_4$, $t_5t_6$, we also know $t_1^2 - t_2^2$,
$t_3^2 - t_4^2$, $t_5^2 - t_6^2$. Thus we can factorize $g(x)$ further
into 6 factors:

$$g(x) =$$

$$\big( (x^2-t_1^2)(x^2+t_2^2) \big) \hspace{5mm}  \big( (x^2-t_3^2)(x^2+t_4^2) \big) \hspace{5mm} \big( (x^2-t_5^2)(x^2+t_6^2) \big) \hspace{5mm}$$

$$\big( (x^2+t_1^2)(x^2-t_2^2) \big) \hspace{5mm} \big( (x^2+t_3^2)(x^2-t_4^2) \big) \hspace{5mm} \big( (x^2+t_5^2)(x^2-t_6^2) \big) \hspace{5mm}$$

Each factor is a polynomial with known coefficients. Each of them is a
quadratic in $x^2$. We can solve each of these quadratics to get
$t_1^2$, $t_2^2$, $t_3^2$, $t_4^2$, $t_5^2$, and $t_6^2$. This helps us
factorize $g(x)$ fully into its $12$ factors of the form
$(x^2 - t_i^2)$.

We can recover $t_i\text{s}$ by taking the square roots. There are two possible
roots in each case. However, once we fix $t_1$, we can find the
corresponding $t_2$ since we already know $t_1t_2$. Similarly, once we
fix $t_3$, we can find the corresponding $t_4$. Similarly for $t_5$ and
$t_6$. The three choices of signs of $t_1, t_3, t_5$ have to be aligned
such that:

$$t_1 + t_3 + t_5 = t_2 + t_4 + t_6$$

But we still need to
distinguish between $(t_1, t_2)$ and $(t_2, t_1)$ and similarly for the
other two pairs. We can do this by observing that:

$$(t_1 + it_2)(t_3 + it_4)(t_5 + it_6)$$

$$=(1+i)^3(r_1+r_2-r_3-r_4)(r_1+r_3-r_4-r_2)(r_1+r_4-r_2-r_3)$$

is symmetric in the roots $r_1$, $r_2$, $r_3$, $r_4$ and is a known
quantity. With these restriction, all $t_i\text{s}$ get fixed. From these, the
roots can be solved using a system of linear equations.

#### Roots of unity with Lagrange Resolvent

We know that the primitive $n^{th}$ root of unity satisfies

$$x^{n-1} + x^{n-2} + \ldots + x^2 + x +1=0$$

We also know that if $n$
is not a prime with $n=ab$ where $1<a<n$ and $1<b<n$, the $n^{th}$
primitive root of unity is a product of the $a^{th}$ primitive root of
unity and $b^{th}$ primitive root of unity. Hence, to solve for primitve
roots of unity, it is enough to consider cases where $n = p$ is a prime.
Lagrange method can be used for solving such polynomials too.

Let the roots of this polynomial be $\alpha$, $\alpha^2$, $\alpha^3$,
$\ldots$, $\alpha^{p-1}$. This can also be arranged in a slightly
different order: $\alpha$, $\alpha^g$, $\alpha^{g^2}$, $\ldots$,
$\alpha^{g^{p-2}}$. This is always possible due to a result in number
theory. There always exists a generator $g$ such that $g^i$ mod($p$)
generates all of $1,2,3,\ldots, p-1$ (mod $p$). The resolvent
corresponding to this order is:

$$t = \alpha + \beta\alpha^g + \beta^2\alpha^{g^2} + \ldots + \beta^{p-2}\alpha^{g^{p-2}}$$

where $\beta$ is a primitive $(p-1)^{th}$ root of unity. We assume
$\beta$ to be a known quantity. The resolvent polynomial would have a
factor of $(x^{p-1} - t^{p-1})$ This factor is not possible to express
in terms of known values for most polynomials (as shown in the case of
the general quartic). But in the case of the above polynomial, this
factor is expressible in terms of known values. In other words, we can
show that $t^{p-1}$ is a known value.

When expanded in terms of $\alpha$ and $\beta$, $t^{p-1}$ can be
expressed as a polynomial in $\alpha$ whose coefficients are polynomials
in $\beta$:

$$t^{p-1} = P_0(\beta) + P_1(\beta)\alpha + P_2(\beta)\alpha^g + P_3(\beta)\alpha^{g^2}+ \ldots + P_{p-1}(\beta)\alpha^{g^{p-2}}$$

When $\alpha$ is replaced by $\alpha^g$ in the above, $t$ becomes
$t/\beta$. The left hand side remains unchanged since
$(t/\beta)^{p-1} = t^{p-1}$. On the right hand side, this implies
$P_1(\beta) = P_2(\beta) = \ldots =P_{p-1}(\beta)$. This gives:

$$t^{p-1} = P_0(\beta) + P_1(\beta)(\alpha + \alpha^g + \alpha^{g^2}+ \ldots + \alpha^{g^{p-2}}) = P_0(\beta) - P_1(\beta)$$

Hence, $t^{p-1}$ is a known quantity.

Once we know $t$ we can solve for the individual roots. Here is a sketch
of the method. Consider the quantities:

$$u_1 = \alpha + \beta\alpha^g + \beta^2\alpha^{g^2} + \ldots + \beta^{p-2}\alpha^{g^{p-2}}$$

$$u_2 = \alpha + \beta^2\alpha^g + \beta^4\alpha^{g^2} + \ldots + \beta^{2(p-2)}\alpha^{g^{p-2}}$$

$$\vdots$$

$$u_k = \alpha + \beta^k\alpha^g + \beta^{2k}\alpha^{g^2} + \ldots + \beta^{k(p-2)}\alpha^{g^{p-2}}$$

$$\vdots$$

$$u_{p-1} = \alpha + \beta^{p-1}\alpha^g + \beta^{2({p-1})}\alpha^{g^2} + \ldots + \beta^{(p-1)(p-2)}\alpha^{g^{p-2}}$$

We see that $u_1 = t$ is a known quantity.
$u_{p-1} = \alpha + \alpha^g + \alpha^{g^2} + \ldots + \alpha^{g^{p-2}} = -1$
is also a known quantity. In general, any $u_iu_1^{p-1-i}$ can be shown
to be a known quantity by the exact same method as in showing $t^{p-1}$
to be a known quantity. Once we find all $u_i\text{s}$, we can solve for
$\alpha$ from the system of linear equations.

# Field and Field Extensions

So far we have observed that, for solving a polynomial:

1.  We can create a new variable $t$ called the resolvent which is
    expressed entirely using the roots of the polynomial and known
    quantities (like coefficients of the polynomial and roots of unity).

2.  $t$ can be expressed as a root of a polynomial with known
    coefficients. We have been able to solve this polynomial so far in
    the cases that we have explored. Though, this is not always
    guaranteed.

3.  Once we find $t$, we can obtain all the roots using known
    quantities.

We can show that points 1 and 3 are always true. There always exists
some resolvent that is entirely expressed using the known coefficients,
and if the resolvent is known, the roots will become known.

#### Lagrange Theorem on Resolvents

Suppose $t$ is an expression in terms of the roots and known quantities,
and $u$ is another expression in roots and known quantities. Let us
evaluate all possible values of $t$ and $u$ under the various root
permutations. If it so happens that the permutations that change the
value of $u$ also change the value of $t$, then $u$ can be expressed
using $t$ and other known values.

**Proof**: Let us assume there are $k$ different values that $u$ can
take under the root permutations: $u_1$, $u_2$, $u_3$, $\ldots$, $u_k$.
Let the corresponding values of $t$ be $t_1$, $t_2$, $t_3$, $\ldots$,
$t_k$. Consider the following:

$$u_1 + u_2 + \ldots + u_k$$

$$t_1u_1 + t_2u_2 + \ldots + t_ku_k$$

$$t_1^2u_1 + t_2^2u_2 + \ldots + t_k^2u_k$$

$$t_1^3u_1 + t_2^3u_2 + \ldots + t_k^3u_k$$ $$\vdots$$

$$t_1^{k-1}u_1 + t_2^{k-1}u_2 + \ldots + t_k^{k-1}u_k$$

All these expressions are symmetric in roots and are known quantities.
Hence, $u_1\text{s}$ can be expressed in terms of $t_i^j$:

$$u_1 = \frac{D_1}{D} = \frac{D_1D}{D^2}$$

where D is the Vandermonde
Determinant, $D^2 = \prod (t_i - t_j)^2$. $D^2$ is symmetric in $t_i$
and hence symmetric in roots and is a known quantity. $D_1$ is same as D
but $t_1^j$ is replaced with the value of the $j^{th}$ expression in the
above system of equations. Hence, the numerator is a polynomial in $t_1$
whose coefficients are symmetric polynomials in $t_2$, $t_3$, $\ldots$,
$t_k$, which in turn can be expressed as polynomials in $t_1$. Hence
$u_1$ can be expressed as polynomial in $t_1$ (with coefficients being
known quantities).

If we can find a $t$ such that it takes $n!$ different values under root
permutations, we have got our resolvent. Indeed, such a resolvent can
always be formed using a linear combination of roots. For such a linear
combination to not change under some requires the coefficients to
satisfy elaborate constraints. Finding all such constraints for our
given polynomial and then choosing coefficients that violate this
constraint provides us the resolvent. We have not only proved the
existence of a resolvent, but also a resolvent that is linear in the
roots.

#### Field

From now on, we consider all known quantities to come from a "Field\". A
field is closed under the basic algebraic operations like addition,
subtraction, multiplication, and division (of non-zero elements).
However, root operations like square roots, cube roots, etc., are not
supported in a field. We represent our field of known quantities as $K$.
$K$ contains the coefficients of the given polynomial and other rational
numbers. We sometimes also assume it to contain roots of unity.

#### Field Extension

Suppose $g(x)$ is a irreducible polynomial with coefficients from $K$.
Let $t$ be a root of $g(x)$. Then, we can introduce a new element $t$ to
the existing field $K$ using an extension. We represent this as $K(t)$.
Since $K \in K(t)$ and $t \in K(t)$, any polynomial in $t$ with
coefficients in $K$ must also be in $K(t)$. We can show that this is
enough to form a new bigger field.

Consider the set of polynomials in some variable with coefficients from
$K$. Two polynomials that have the same remainder on division by $g(x)$
is considered to be equal. Addition, subtraction and multiplication of
two such polynomials would result in another polynomial.

For defining division, we need to first define a way to effectively
invert any $a \in K(t)$ such that $a(x)b(x) = g(x)h(x) + 1$. We can
always find this using Euclid's method of finding GCD of two
polynomials. Since $g(x)$ is irreducible, the GCD of $a, g$ is $1$.
Hence all elements of $K(t)$ can be inverted and we can define division
as multiplication by the inverse.

#### Splitting Field

The field extension $K(r_1, r_2, \ldots, r_n)$ which contains all the
roots of the polynomial of the polynomial is called the Splitting Field.
Solving a polynomial is equivalent to finding the Splitting Field. From
Lagrange's Theorem on Resolvents, we have shown that there is always a
$t$ such that $K(t) = K(r_1, r_2, \ldots, r_n)$. The resolvent
polynomial $g$ which has coefficients in $K$ and takes $t$ as a root is
easy to find. We start with a polynomial whose roots are $t$ and all its
values under the root permutations. This polynomial has coefficients in
$K$. We factorize this polynomial into irreducible factors. The
irreducible factor that contains $t$ as a root is our resolvent
polynomial $g$.

# Root Permutations

Let us revisit the solution to the quartic using the Lagrange Resolvent
by following the irreducible polynomial containing $t_1$ as a root.

Initially, the resolvent polynomial is the full $24$ degree polynomial.
The only expression in roots that can be evaluated (as a known quantity)
at this point are the ones that remain unchanged by all the $24$
permutations of the roots.

In the next step, we end up knowing the values of $r_1r_2 + r_3r_4$,
$r_1r_3 + r_2r_4$, $r_1r_4 + r_2r_3$. These are not symmetric
polynomials in roots. The root permutations that preserve them are:

1.  $(r_1, r_2, r_3, r_4)$: The identity permutation

2.  $(r_2, r_1, r_4, r_3)$: Flip $(r_1, r_2)$ and Flip $(r_3, r_4)$

3.  $(r_3, r_4, r_1, r_2)$: Flip $(r_1, r_3)$ and Flip $(r_2, r_4)$

4.  $(r_4, r_3, r_2, r_1)$: Flip $(r_1, r_4)$ and Flip $(r_2, r_3)$

At this point, the resolvent polynomial is $(x^2-t_1^2)(x^2+t_2^2)$.
These permutations take $t_1$ to the other roots of the resolvent
polynomial: $t_1$, $-t_1$, $it_2$, $-it_2$. The only expressions in
roots that we can evaluate at this stage happens to be polynomials in
roots that remain unchanged by these permutations. For example, we can
evaluate $(r_1 + r_2)(r_3 + r_4)$.

In the next step, we end up knowing the values of $r_1r_2$, $r_3r_4$.
The root permutations that preserve them, among the permutations listed
above are:

1.  $(r_1, r_2, r_3, r_4)$: The identity permutation

2.  $(r_2, r_1, r_4, r_3)$: Flip $(r_1, r_2)$ and Flip $(r_3, r_4)$

At this point, the resolvent polynomial is $(x^2-t_1^2)$. These
permutations take $t_1$ to the other roots of the resolvent polynomial:
$t_1$, $-t_1$. The only expressions in roots that we can evaluate at
this stage happens to be polynomials in roots that remain unchanged by
these permutations. For example, we can evaluate $(r_1 + r_2)$ and
$(r_3 + r_4)$.

We notice the following pattern:

1.  Polynomial expressions in roots that can be evaluated as known
    quantities are characterized by a subset of root permutations.

2.  These same permutations map one root of the resolvent polynomial to
    the other roots of the resolvent polynomial.

#### Root Permutations

$K(t) = K(r_1,r_2,\ldots,r_n)$. $t$ is a root of $g(x)$. Once we solve
for $t$, we can recover the roots $r_1$, $r_2$, $\ldots$, $r_n$, using a
polynomial in $t$:

$$r_1 = \phi_1(t), r_2 = \phi_2(t), \ldots r_n = \phi_2(t)$$

However, we cannot distinguish one root of $g(x)$ from another. Thus, if
$t'$ is another root of $g(x)$, we could also get a permutation of the
roots $r_1$, $r_2$, $\ldots$, $r_n$ as

$$\phi_1(t'), \phi_2(t'), \ldots \phi_2(t')$$

We can show that this is a permutation of $r_1$, $r_2$, $\ldots$, $r_n$.
$f(\phi_i(x))$ has $t$ as a root. Then, the irreducible polynomial of
$t$: $g(x)$ divides $f(\phi_i(x))$. So, $f(\phi_i(t')) = 0$ establishing
that $\phi_1(t')$, $\phi_2(t')$, $\ldots$, $\phi_2(t')$ are all roots of
$f(x)$.

Further, if $\phi_i(t') = \phi_j(t')$, then the polynomial
$\phi_i(x) - \phi_j(x)$ has a root $t'$. Then $g(x)$ $|$
$\phi_i(x) - \phi_j(x)$ and $t$ is also a root of
$\phi_i(x) - \phi_j(x)$ implying that $\phi_i(t) =  \phi_j(t)$, which is
not possible.

Thus, $\phi_1(t')$, $\phi_2(t')$, $\ldots$, $\phi_2(t')$ is a
rearrangement of $r_1$, $r_2$, $\ldots$, $r_n$. If we were to somehow
know $t$, and then find the roots $r_i$ using $t$, we will not be able
to distinguish between these permutations. Let us call these
permutations as "**Allowed Permutations**\".

#### Polynomial Expressions in Roots that can be evaluated

Consider any polynomial expression in roots:
$\psi(r_1, r_2,\ldots,r_n)$. We know that

$$\psi(r_1, r_2,\ldots,r_n) = \psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) \in K(t)$$

since it is a polynomial in $t$. Suppose that we can evaluate this
expression, and assign it a known value. Then,
$\psi(r_1, r_2,\ldots,r_n) = c \in K$. This implies that $t$ is a root
of $\psi(\phi_1(x), \phi_2(x), \ldots, \phi_n(x)) - c =0$. Again, by the
same logic as before, any $t'$ is also a root of
$\psi(\phi_1(x), \phi_2(x), \ldots, \phi_n(x)) - c = 0$. This shows that
if we can evaluate any polynomial expression in roots, then it is
invariant under all the allowed root permutations.

The converse of this statement is also true. If we have an expression
that is invariant under all the allowed root permutations, then it can
be assigned a value from $K$. Suppose $\psi$ is invariant to all the
allowed permutations. Then,

$$\psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) = \psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t))$$

$$\psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) = \psi(\phi_1(t'), \phi_2(t'), \ldots, \phi_n(t'))$$

$$\psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) = \psi(\phi_1(t''), \phi_2(t''), \ldots, \phi_n(t''))$$

$$\vdots$$

$$\psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) = \psi(\phi_1(t^{(k)}), \phi_2(t^{(k)}), \ldots, \phi_n(t^{(k)}))$$

Adding them all up, we get:

$$\psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t)) = \frac{1}{k+1} \sum_{i=0}^{k} \psi(\phi_1(t^{(i)}), \phi_2(t^{(i)}), \ldots, \phi_n(t^{(i)}))$$

The right side of this expression is symmetric in the roots of $g(x)$.
Hence, it is a known quantity.

Thus the only expressions that can be evaluated as a known quantity are
those that are invariant under the allowed root permutations.

# Group of Root Permutations

So far we have found that:

1.  $K(t) = K(r_1, r_2, \ldots, r_n)$. Solving for the roots of the
    polynomial $f(x)$ is equivalent to solving for the resolvent
    polynomial $g(x)$. $t$ is a polynomial in the roots:
    $t = \psi(r_1, r_2, \ldots, r_n)$ and the roots $r_i$ can be
    expressed as a polynomial in $t$: $r_i = \phi_i(t)$. $g(x)$ is the
    polynomial whose roots are $t$ and its variants under all the root
    permutations of $r_i\text{s}$. If $g$ can be factorized, we take the
    irreducible factor containing the root $t$ and call it $g$.

2.  At this point, if we were to somehow solve $g(x)$ and get $t$, it
    could be any one of the roots $t'$ of $g(x)$. If we then were to
    recover the roots of $f(x)$ using
    $(\phi_1(t'),\phi_2(t'),\ldots,\phi_n(t'))$, it would be a
    permutation of $r_i\text{s}$.

3.  A polynomial in $r_i\text{s}$ can be evaluated to a known quantity in $K$
    if and only if it is invariant to these same permutations.

Since $t$ can be expressed as polynomials in roots $r_i$:
$t = \psi(r_1, r_2, \ldots, r_n)$, we would like to know what effect
does the allowed permutations have on $t$.

#### Extending Root Permutations to Automorphisms

In the last section, we looked at each element of the extended field as
an element of $K(t)$. However, these can also be looked at as elements
of $K(r_1,r_2,\ldots,r_n)$. Given any allowed permutation $\sigma$, we
can apply it on the entire field, since any element of the field can be
expressed as a polynomial in $r_1,r_2,\ldots,r_n$ with coefficients from
$K$. By defining $\sigma(k) = k$ for all $k \in K$ and
$\sigma(r_i) = \phi_i(t')$. For any element $u \in K(t)$, we can define:

$$\sigma(u) = \sigma(\psi(r_1,r_2,\ldots,r_n)) = \psi(\sigma(r_1),\sigma(r_2),\ldots,\sigma(r_n))$$

This is equivalent to saying:

$$\sigma(\psi(\phi_1(t),\phi_2(t),\ldots,\phi_n(t))) = \psi(\phi_1(t'),\phi_2(t'),\ldots,\phi_n(t'))$$

For this definition to be well-defined, we need that, whenever we have
two different representations of $u$, $\sigma(u)$ defined using either
of those representations is consistent with the other. If:

$$u = \psi_1(r_1,r_2,\ldots,r_n)=\psi_2(r_1,r_2,\ldots,r_n)$$

Then,

$$\psi_1(r_1,r_2,\ldots,r_n) - \psi_2(r_1,r_2,\ldots,r_n) = 0$$

$$\psi_1(\phi_1(t),\phi_2(t),\ldots,\phi_n(t)) - \psi_2(\phi_1(t),\phi_2(t),\ldots,\phi_n(t)) = 0$$

This can be seen as a polynomial with coefficients in $K$ with a root
$t$. It immediately follows that $t'$ is also a root of the same
polynomial:

$$\psi_1(\phi_1(t'),\phi_2(t'),\ldots,\phi_n(t')) - \psi_2(\phi_1(t'),\phi_2(t'),\ldots,\phi_n(t')) = 0$$

which is same as saying:

$$\sigma(\psi_1(\phi_1(t),\phi_2(t),\ldots,\phi_n(t))) = \sigma(\psi_2(\phi_1(t),\phi_2(t),\ldots,\phi_n(t)))$$

It is easy to see that the mapping $\sigma$ preserves sums and products:

$$\sigma(x + y) = \sigma(x) + \sigma(y)$$

$$\sigma(xy) = \sigma(x)\sigma(y)$$

where $x, y$ are polynomials of the
form $\psi(r_1,r_2,\ldots,r_n)$. Hence $\sigma$ defined this way is an
automorphism from $K(r_1,r_2,\ldots,r_n)$ to itself.

#### Effect on roots of resolvent polynomial

We would like to know how $\sigma$ acts on $t$, $t'$ and other roots of
$g(x)$. To begin with, we can verify that, $\sigma(t) = t'$. If
$t = \psi(r_1, r_2, \ldots, r_n)$:

$$t = \psi(r_1, r_2, \ldots, r_n) = \psi(\phi_1(t), \phi_2(t), \ldots, \phi_n(t))$$

$t$ is a root of
$\psi(\phi_1(x), \phi_2(x), \ldots, \phi_n(x)) - x = 0$. And hence any
other $t'$ is a root of the same polynomial giving:

$$t' = \psi(\phi_1(t'), \phi_2(t'), \ldots, \phi_n(t'))$$

$$t' = \psi(\sigma(r_1), \sigma(r_2), \ldots, \sigma(r_n))$$

This shows that the allowed permutation derived from $t \to t'$ maps $t$
to $t'$. For exploring the effect of $\sigma$ on any root other than $t$
(say $t'$), we know that:

$$g(t') = 0$$ $g(x)$ is a polynomial with

coefficients in $K$, we can apply $\sigma$ on $g(x)$ to get
$\sigma(g(x)) = g(\sigma(x))$. This happens because $\sigma$ leaves any
element of $K$ unchanged. Hence, applying $\sigma$ on both sides:

$$g(\sigma(t')) = 0$$

This shows that $\sigma(t')$ is also a root of
$g(x)$. Thus, the allowed root permutations map one root of $g$ to
another. $\sigma(t) = t'$, $\sigma(t') =t''$ and so on.

#### Group

We can use this fact to compose two allowed root permutations on top of
each other to get another allowed root permutation. We can interpret the
first permutation as moving $t \to t'$ and the second one as moving
$t' \to t''$. Since, the permutation $t \to t''$ is also a allowed root
permutation, we know that composition of two allowed permutations gives
another allowed permutation.

A set of permutations that contains the identity permutation and are
closed under compositions is called a Group. The group of allowed root
permutations is called the Galois Group: $\text{Gal}(K(r_1,r_2,\ldots,r_n)/K)$.
The size of the Galois Group = degree of $g(x)$. All possible $n!$
permutations form the permutation group $S_n$. Galois Group is a
subgroup of $S_n$.

#### Examples of Galois Groups

For the general polynomial of $n^{th}$ degree, the Galois Group contains
all the $n!$ permutations.

For some special polynomials like the ones for the $p$-th root of unity,
the roots have more algebraic structure in them. The roots are of the
form $\alpha$, $\alpha^2$, $\alpha^3$, $\ldots$, $\alpha^{p-1}$. Any
allowed root permutation is a automorphism that preserves algebraic
structure. Thus defining $\sigma(\alpha) = \alpha^i$ defines the root
permutation for all the other roots, since for any other root
$\alpha^j$, $\sigma(\alpha^j) = \sigma(\alpha)^j = \alpha^{ij}$. In this
way, there are only $p$ possible root permutations:
$\sigma(\alpha) = \alpha^i$ where $i = 1,2,3\ldots,p$.

#### Fixed Points

From the previous section, we know that if any polynomial expression in
roots in invariant to all the root permutations, then it must be in $K$.
This can also be stated as: If for some $x \in K(r_1,r_2,\ldots,r_n)$,
$\sigma(x) = x$ for all $\sigma$ from the Galois Group, then $x \in K$.

# Groups and Subgroups

What we know so far:

1.  There are two ways to look at the field containing the roots of
    $f(x)$: $K(r_1, r_2, \ldots, r_n) = K(t)$. If we were to ever solve
    the resolvent polynomial $g(x)$, and try to recover the roots
    $(r_1,r_2,\ldots,r_n)$ of $f(x)$, we would not be able to
    distinguish between the different roots of $g(x)$ or equivalently
    certain permutations of $(r_1,r_2,\ldots,r_n)$. These permutations
    of $(r_1,r_2,\ldots,r_n)$ form a group called the Galois Group.

2.  Any $\sigma \in \text{Gal}(K(r_1,r_2,\ldots,r_n)/K)$, can be extended to
    an Automorphism on $K(r_1,r_2,\ldots,r_n)$ that leaves $K$
    unchanged. Equivalently, any polynomial in $(r_1,r_2,\ldots,r_n)$
    that can be evaluated as a known quantity in $K$ is invariant to the
    permutations in the Galois Group.

3.  The converse is also true. If there is any
    $x \in K(r_1,r_2,\ldots,r_n)$ such that $\sigma(x)=x$ for all
    $x \in \text{Gal}(K(r_1,r_2,\ldots,r_n)/K)$, then $x \in K$.
    Equivalently, if a polynomial in $(r_1,r_2,\ldots,r_n)$ is invariant
    to all permutations in the Galois Group, then, the polynomial can be
    evaluated as a known quantity in $K$.

We know from our experience of solving cubic and quartic equations that
the process of solving the polynomial involves extending the field $K$
with appropriate entity $r$ to enable factorizing $g(x)$ into a
polynomial $h(x)$ of a smaller degree, such that $h(x)$ has $t$ as a
root and is irreducible in $K(r)$. We would like to know how the
$\text{Gal}(K(r_1,r_2,\ldots,r_n)/K(r))$ relates to the 
$\text{Gal}(K(r_1,r_2,\ldots,r_n)/K)$.

Here is an example. For the cubic equation, we start with the resolvent
polynomial which is of degree $6 = 3!$ with (say $t=t_1$):

$$t_1 = r_1 + \omega r_2 + \omega^2 r_3$$

$$t_2 = r_3 + \omega r_1 + \omega^2 r_2$$

$$t_3 = r_2 + \omega r_3 + \omega^2 r_1$$

$$t_4 = r_1 + \omega r_3 + \omega^2 r_2$$

$$t_5 = r_2 + \omega r_1 + \omega^2 r_3$$

$$t_6 = r_3 + \omega r_2 + \omega^2 r_1$$

The irreducible resolvent
polynomial is:

$$g(x) = \prod_{i=1}^{6}(x - t_i)$$

After we are able to extend the field with:

$$
\begin{align*}
t_1^3 = (r_1 + \omega r_2 + \omega^2 r_3)^3 &= r_1^3 + r_2^3 + r_3^3 + 6r_1 r_2 r_3 \\&
+3\omega(r_1^2r_2+r_2^2r_3+r_3^2r_1)\\
&+3\omega^2(r_1r_2^2+r_2r_3^2+r_3r_1^2)\\    
t_4^3 = (r_1 + \omega^2 r_2 + \omega r_3)^3 &= r_1^3 + r_2^3 + r_3^3 + 6r_1 r_2 r_3 \\&
+3\omega^2(r_1^2r_2+r_2^2r_3+r_3^2r_1)\\
&+3\omega(r_1r_2^2+r_2r_3^2+r_3r_1^2)
\end{align*}
$$

We are able to factorize $g(x)$ into the two irreducible
factors $h(x) = h_1(x) = (x^3-t_1^3)$ and $h_2(x) = (x^3-t_4^3)$. The
Galois Group corresponding to $g(x)$ is $S_3$, the entire permutation
group of all $3! = 6$ permutations. For the resolvent polynomial $h(x)$
we have the roots:

$$t_1 = r_1 + \omega r_2 + \omega^2 r_3$$

$$t_2 = r_3 + \omega r_1 + \omega^2 r_2$$

$$t_3 = r_2 + \omega r_3 + \omega^2 r_1$$

The Galois Group corresponding
to $h(x)$ contains $3$ permutations: The permutation of $(r_1,r_2,r_3)$
into one of $\{(r_1,r_2,r_3),(r_3,r_1,r_2),(r_2,r_3,r_1)\}$. These
permutations correspond to $t_1 \to t_1$, $t_1 \to t_2$, and
$t_1 \to t_3$. We observe that these same permutations map
$t_4 \to t_4$, $t_4 \to t_6$, and $t_4 \to t_5$.

#### Subgroup

When $g(x)$ can be factorized into a smaller irreducible polynomial
$h(x)$, such that $t$ is a root of $h(x)$, we can define a new Galois
Group corresponding to $\text{Gal}(K(r_1,r_2,\ldots,r_n)/K(r))$. This new
Galois Group maps roots of $h(x)$ to itself. These permutations can then
be extended onto the entire field $K(r_1,r_2,\ldots,r_n)$ such that it
leaves $K(r)$ unchanged.

The factors of $g(x)$ are polynomials with coefficients from $K(r)$.
Suppose $h_2(x)$ is one such factor. Let $\sigma$ be a permutation from
the new Galois Group. Then, $\sigma(h_2(x)) = h_2(\sigma(x))$. Thus
$\sigma$ maps roots of each factor of $g$ onto other roots of the same
factor. In this way, it maps all roots of $g(x)$ to other roots of
$g(x)$. This proves that the Galois Group corresponding to $h(x)$ is a
subgroup of the Galois Group corresponding to $g(x)$.

#### Cosets and Size of Subgroup

Let us denote:

$$G := \text{Gal}(K(r_1,r_2,\ldots,r_n)/K)$$

$$H := \text{Gal}(K(r_1,r_2,\ldots,r_n)/K(r))$$

$H$ is a subgroup of $G$. For any $\sigma \notin H$, consider the set of permutations
$\sigma H = \{\sigma h$ $|$ $h \in H\}$. Let $\sigma(t) = t'$. $t'$ is a
root of $g(x)$, but not $h(x)$. Let $t'$ be root of $h_2(x)$. $\sigma H$
consists of permutations that map $t \to t' \to t''$. We know that
permutations in $H$ map roots of $h_2(x)$ to another root of $h_2(x)$.
Thus, $t''$ is also a root of $h_2(x)$. $\sigma H$ can then be
interpreted as permutations that map $t$ to some root $t''$ of $h_2(x)$.

If two permutations of $\sigma H$ map $t$ to the same root of $h_2(x)$,
then we have $\sigma h_1 = \sigma h_2$ for some permutations
$h_1, h_2 \in H$. But we can then compose $\sigma^{-1}$ to both of them
to get $h_1 = h_2$ which is not possible. Thus, $\sigma H$ has just as
many permutations as $H$. This also means that $h_2(x)$ has just as many
roots as $h(x)$.

We can repeat this process till we have covered all the permutations in
$G$, and equivalently, all the irreducible factors of $g(x)$. Each of
these factors would have the same degree. The sets $\sigma H$ are called
right-cosets of $H$. They all are of the same size. This shows that size
of $H$ divides the size of $G$. This process of dividing a group into
cosets of its subgroup can be carried for any group. In general, the
size of a subgroup always divides the size of the group.

# Factorization by Field Extension

So far, we have looked at the under the effect of factorization of the
resolvent polynomial. We found that Galois Group $H$ is a subgroup of
$G$. The size of $H$, which is equal to the degree of $h(x)$ divides the
size of $G$, which in turn is equal to the degree of $g(x)$.

In this section, we explore the factorization from the point of view of
the Field Extension. Suppose that we extend the field $K$ with an
element $r$ such that $r^p \in K$. We take $p$ to be a prime number. In
the example of the cubic solution from previous section, we can show
that the extension is using
$r = (r_1+\omega r_2+\omega^2 r_3)^3 - (r_1+\omega^2 r_2+\omega r_3)^3$
and that $r^2 \in K$. Another equivalent choices for $r$ is
$r = (r_1r_2^2 + r_2r_3^2 + r_3r_1^2) - (r_1^2r_2 + r_2^2r_3 + r_3^2r_1)$.

#### Polynomial Factorization

Let $U(x)$ be a polynomial with coefficients in $K(r)$. Then, $U$ can
also be expressed as a polynomial with coefficients in $K$:

$$U(x, r) =  a_m(r) x^m +a_{m-1}(r) x^{m-1} +\ldots +a_2(r)x^2 +a_1(r)x +a_0(r)$$

If $V(x, r)$ is a factor of $U(x, r)$, then $U(x, r) = V(x, r)W(x, r)$

$$U(x, r) - V(x,r)W(x,r) =  b_m(r) x^m +b_{m-1}(r) x^{m-1} +\ldots +b_2(r)x^2 +b_1(r)x +b_0(r) = 0$$

$r$ is a root of $b_m(y)$, $b_{m-1}(y)$, $\ldots$, $b_2(y)$, $b_1(y)$,
$b_0(y)$. The irreducible polynomial of $r$ over $K$ is $y^p - r^p = 0$.
Thus, $y^p - r^p$ divides all of these polynomials. This means:

$$U(x, r) = V(x,r)W(x,r)$$

$$U(x, \alpha r) = V(x,\alpha r)W(x,\alpha r)$$

$$U(x, \alpha^2 r) = V(x,\alpha^2 r)W(x,\alpha^2 r)$$ $$\vdots$$

$$U(x, \alpha^{p-2} r) = V(x,\alpha^{p-2} r)W(x,\alpha^{p-2} r)$$

$$U(x, \alpha^{p-1} r) = V(x,\alpha^{p-1} r)W(x,\alpha^{p-1} r)$$

What we have shown is that if $V(x, r)$ divides $U(x, r)$, then
$V(x, \alpha^i r)$ divides $U(x, \alpha^i r)$.

#### Factorization of Resolvent Polynomial

If $U(x, r) = g(x)$, and $h(x, r)$ is a factor of $U(x, r) = g(x)$, then
all of $h(x, \alpha^i r)$ are factors of $U(x, \alpha^i r) = g(x)$.
Thus, we can find $p$ different factors of $g$ from a single factor
$h(x, r)$:

$$g(x) = h(x,r)q(x,r)$$

$$g(x) = h(x,\alpha r)q(x,\alpha r)$$

$$g(x) = h(x,\alpha^2 r)q(x,\alpha^2 r)$$ $$\vdots$$

$$g(x) = h(x,\alpha^{p-1} r)q(x,\alpha^{p-1} r)$$

Multiplying these, we would get:

$$g(x)^p = H(x)Q(x)$$

where:

$$H(x) = h(x, r)h(x, \alpha r)h(x, \alpha^2 r) \ldots h(x, \alpha^{p-1} r)$$

This would imply:

$$g(x)^j = H(x) = h(x, r)h(x, \alpha r)h(x, \alpha^2 r) \ldots h(x, \alpha^{p-1} r)$$

There are $p$ irreducible factors on the right which when multiplied
give $g(x)^j$. From the previous section, we know that each of these
factors has a degree that divides the degree of $g(x)$. If deg($h(x)$) =
deg($g(x)$)/$m$, then $m$ factors on the right would be of the same
degree as $g(x)$ and $mj$ factors on the right side makes the total $p$
factors. This implies that $m$ divides $p$ or $m=1$ or $m=p$. Thus
deg($h(x)$) = deg($g(x)$)/$p$ or deg($h(x)$) = deg($g(x)$).

We have $p$ irreducible factors whose product is $g(x)^j$. From the
previous section, we know that degree of $h(x)$ divides degree of
$g(x)$. Let deg $h(x)$ = deg $g(x)/m$. Then, it takes exactly $m$ such
factors to match the degree of $g(x)$. However, the left side has $j$
copies of $g(x)$. It follows that the total number of irreducible
factors on the right must be $mj$. But, we know this to be $p$, which is
prime. Thus $m=1, j=p$ or $m=p,j=1$. This gives either deg $h(x)$ = deg
$g(x)$ implying $h(x) = g(x)$ or deg $h(x)$ = deg $g(x)/p$ implying:
$$g(x) = h(x, r)h(x, \alpha r)h(x, \alpha^2 r) \ldots h(x, \alpha^{p-1} r)$$

#### Normal Subgroup

Let $t'$ be a root of $h(x,r)$. We know that $t'$ can be written as a
polynomial in $t$: $t' = \psi(t)$. Since $h(\psi(t),r) = 0$, we know
that $t$ is a root of $h(\psi(x),r)$. Thus $h(x,r)$ is a factor of
$h(\psi(x),r)$.

$h(x,r)$ being a factor of $h(\psi(x),r)$, implies that,
$h(x,\alpha^i r)$ is a factor of $h(\psi(x),\alpha^i r)$. Thus, if $u$
is a root of $h(x,\alpha^i r)$, so is $\psi(u)$.

The mapping $t \to \psi(t)$ can be represented using a permutation in
$H$. The permutation that maps $t \to u$ (say for $\sigma \in G$) also
maps $\psi(t) \to \psi(u)$. Thus, the mapping
$t \to \psi(t) \to \psi(u)$ can be seen as a permutation in $H$ followed
by some appropriate permutation in $G$. Thus, the permutation from $t$
to the roots of each factor can be represented as left-cosets $H\sigma$.

We already know that these same permutations can be presented as
right-cosets $\sigma H$. It is a rare occurence that right-cosets are
also left-cosets:

$$\sigma H = H\sigma$$

This relation holds true for
all $\sigma$. In other words for any $\sigma \in G$:

$$\sigma^{-1} H\sigma =H$$

Whenever, a subgroup follows this property,
we call it a Normal Subgroup.

# Solvability

So far, we have shown that, when we extend the field $K$ with a $r$ such
that $r^p \in K$, and the resolvent polynomial $g(s)$ can be further
factorized:

1.  $g(x)$ can be factored into $p$ factors of the form
    $h(x, \alpha^i r)$

2.  The Galois Group $H$ is a normal subgroup of $G$. Size of $H$ is
    $1/p$ times that of $G$.

The opposite of this statement is also true. Suppose we have a field $K$
and Galois Group $G$, that leaves $K$ fixed. If we know a normal
subgroup of $G$, say $H$ with size $1/p$, then there exists an element
$r$ such that $r^p \in K$, and Galois Group that leaves $K(r)$ fixed is
$N$. Before we prove this statement, we explore the consequences of
having a Normal Subgroup of size $1/p$ times the Group.

#### Quotient Group

For any normal subgroup $H$ of $G$, the cosets form a group defined by:

$$(Ha) (Hb) = (Ha) (bH) = H(abH) = H(Hab) = Hab$$

This is represented by
the Quotient Group $G/H$. We observe that the Quotient Group is of size
$p$.

#### Cyclic Subgroup

For any group $G$, and $\sigma \in G$, we can form the subgroup:

$$\{e, \sigma, \sigma^2,\ldots,\sigma^{k-1}\}$$

where $k$ is the
smallest number such that $\sigma^k = e$. This subgroup is called the
cyclic subgroup generated by $\sigma$ and has size $k$.

#### Quotient Group is Cyclic

Let us consider a cyclic subgroup of the Quotient Group, generated by
some $\sigma \in G/H$. Let the size of the subgroup be $k$. Quotient
Group is of size $p$. So, $k$ divides $p$ implying $k=1$ or $k=p$. $k=1$
is not possible, since the subgroup has atleast two elements $e$ and
$\sigma$. $k=p$ implies that the entire group is the subgroup. This
means that the entire group ends up becoming cyclic, irrespective of the
choice of $\sigma$ (as long as, it is not the identity element).

For our purpose, let us represent the cosets as $H$, $\sigma H$,
$\sigma^2 H$, $\ldots$, $\sigma^{p-1} H$. As a consequence, any
permutation in $G$ can be represented as $\sigma^i h$, where $h \in H$.
We can equivalently define the quotient group $G/H$ as $\{e$, $\sigma$,
$\sigma^2$, $\ldots$, $\sigma^{p-1}\}$.

#### Solvability

Here is a sketch of the proof of the converse statement, inspired from
Lagrange Resolvents. Given $H$, we can obtain all the corresponding $t\text{s}$
by composing the permutations in $H$ on $t$. Let these be $t_1$, $t_2$,
$\ldots$, $t_m$ with $mp = n$ where $n$ is the degree of $g$. We can
obtain the polynomial $h(x) = (x-t_1)(x-t_2)\ldots(x-t_m)$. The
coefficients of this polynomial cannot be in $K$, since the irreducible
polynomial containing $t$ is of degree $n = mp$. The coefficients of
this polynomial are candidates in $K(r)$. Let us choose one such
candidate and call it $\theta_1$.

Consider any permutation $\sigma$ from the quotient group $G/H$.
$\sigma$ fixes only elements of $K$, but not $K(r)$. $\sigma(\theta_1)$
cannot be $\theta_1$, since $\theta_1 \notin K$. Let
$\sigma(\theta_1) = \theta_2$, $\sigma^2(\theta_1) = \theta_3$ and so on
till $\sigma^{p-1}(\theta_1) = \theta_p$. Consider:

$$r_1 = \theta_1 + \alpha\theta_2 + \alpha^2\theta_3 + \ldots + \alpha^{p-1}\theta_p$$

$$r_2 = \theta_1 + \alpha^2\theta_2 + \alpha^4\theta_3 + \ldots + \alpha^{2(p-1)}\theta_p$$

$$r_3 = \theta_1 + \alpha^3\theta_2 + \alpha^6\theta_3 + \ldots + \alpha^{3(p-1)}\theta_p$$

$$\vdots$$

$$r_{p-1} = \theta_1 + \alpha^{p-1}\theta_2 + \alpha^{2(p-1)}\theta_3 + \ldots + \alpha^{(p-1)(p-1)}\theta_p$$

Atleast one of these has to be non-zero. If not, we can sum it all up to
get:

$$0 = p\theta_1 - \theta_1 - \theta_2 - \theta_3 \ldots - \theta_p$$

$$p\theta_1  = \theta_1 + \theta_2 + \theta_3 +\ldots + \theta_p$$

We could apply $\sigma$ on both sides to get:
$\theta_1 = \theta_2 = \ldots = \theta_p$, which is not true. Thus,
without loss of generality we can assume that the following is non-zero:

$$r = \theta_1 + \alpha\theta_2 + \alpha^2\theta_3 + \ldots + \alpha^{p-2}\theta_{p-1} + \alpha^{p-1}\theta_p$$

Applying $\sigma$ on both sides:

$$\sigma(r) = \theta_2 + \alpha\theta_3 + \alpha^2\theta_4 + \ldots + \alpha^{p-2}\theta_{p} + \alpha^{p-1}\theta_1 = r/\alpha$$

This gives $\sigma(r^p) = r^p$.

Any permutation in $G$, is of the form $\sigma^i h$ for some $h \in H$.

$$\sigma^i(h(r)) = \sigma^i(r) = r/\alpha^i$$

since $h$ does not affect
$\theta_i\text{s}$. Hence:

$$\sigma^i(h(r^p)) = (r/\alpha^i)^p = r^p$$

$r^p$ is
fixed by all permutations in $G$, implying that $r^p \in K$.

We have shown that the process of solving a polynomial by radicals can
be understood entirely through the structure of its Galois group. Each
time the base field $K$ is extended by adjoining an element $r$ with
$r^p \in K$, the corresponding resolvent polynomial factors in a highly
structured way, and the Galois group shrinks to a normal subgroup of
size $1/p$ of the original Galois Group. Conversely, the existence of a
normal subgroup guarantees the existence of a field extension obtained
by adjoining a $p^{th}$ root. Following this trail of Galois Groups from
that of the original polynomial to a group of only identity element, we
can derive a way to solve the polynomial using basic algebraic
operations and radicals.
